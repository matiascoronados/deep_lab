{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46uESFa1Shq6"
   },
   "source": [
    "### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNLmW5LIStb6"
   },
   "source": [
    "Importar las bibliotecas que son usadas en el desarrollo del laboratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Bibliotecas propias\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os,os.path,math,scipy,cv2\n",
    "import PIL\n",
    "\n",
    "from datasets import ClassLabel\n",
    "\n",
    "#Bibliotecas para LeNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se setean las seeds\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TZd8RYTlS4rb"
   },
   "outputs": [],
   "source": [
    "### Escribir en estos bloques el codigo de acorde a las instrucciones planteadas\n",
    "### pero no olvide que puede crear los bloques que quiera y puede agregar las sub secciones \n",
    "### que deseen, esto es solo una guía basica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIdsSOfqS7XW"
   },
   "source": [
    "Lectura del conjunto de datos, deberá diseñar una estrategia para la selección y lectura del sub conjunto de datos, es decir, se debe crear un conjuntos de datos pequeño que contenga entre 3 a 5 clases y una cantidad de $N$ ejemplos para cada clase, con el fin de tener un conjunto de datos balanceado con un tamaño que se pueda trabajar con los recursos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_amount_of_n_images(img_dir_list, N):\n",
    "    for img_dir in img_dir_list:\n",
    "        img_count = len([name for name in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, name))])\n",
    "        if(img_count < N):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_images(data_frame ,img_label,img_dir):\n",
    "    count = 0\n",
    "    trans = transforms.Compose([transforms.ToTensor()])\n",
    "    for img in os.listdir(img_dir):\n",
    "        if(count < N):\n",
    "            path = os.path.join(img_dir,img)\n",
    "            img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img,(TAM_MAX,TAM_MAX))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #img = PIL.Image.open(path)\n",
    "            #img = img.resize((TAM_MAX,TAM_MAX))\n",
    "            img = trans(img)/255\n",
    "            #img = img[:3]\n",
    "            \n",
    "            data_frame.append((img,img_label))\n",
    "            count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Direccion para carpetas de clases\n",
    "chihuahua_dir = 'images/n02085620-Chihuahua'\n",
    "japanese_spaniel_dir = 'images/n02085782-Japanese_spaniel'\n",
    "maltese_dir = 'images/n02085936-Maltese_dog'\n",
    "pekinese_dir = 'images/n02086079-Pekinese'\n",
    "toy_terrier_dir = 'images/n02087046-toy_terrier'\n",
    "aux_dir_list = [chihuahua_dir,japanese_spaniel_dir,maltese_dir,pekinese_dir,toy_terrier_dir]\n",
    "\n",
    "#Se define la cantidad N de ejemplos por clase\n",
    "N = 140\n",
    "\n",
    "#Se define el tamaño maximo de imagen\n",
    "TAM_MAX = 224\n",
    "\n",
    "#Data frame para almacenar las tuplas (imagen,label)\n",
    "data_frame = []\n",
    "\n",
    "#Se modifica el nombre de los labels\n",
    "labels_names = ['chihuahua','japanese_spaniel','maltese','pekinese','toy_terrier']\n",
    "\n",
    "#Se verifica que todos los directorios tengan la misma cantidad N de imagenes.\n",
    "if(same_amount_of_n_images(aux_dir_list,N)):\n",
    "    #Se almacenan las imagenes, y los labels\n",
    "    storage_images(data_frame,1,chihuahua_dir)\n",
    "    storage_images(data_frame,2,japanese_spaniel_dir)\n",
    "    storage_images(data_frame,3,maltese_dir)\n",
    "    storage_images(data_frame,4,pekinese_dir)\n",
    "    storage_images(data_frame,5,toy_terrier_dir)\n",
    "else:\n",
    "    print('N invalido')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fq4HhEI-TICF"
   },
   "source": [
    "Realizar la división de los datos en dos conjuntos de datos, Train (70%) y Test (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "valid_ratio = 0.1\n",
    "\n",
    "n_train_examples = int(len(data_frame) * train_ratio)\n",
    "n_test_examples = len(data_frame) - n_train_examples\n",
    "n_validate_examples = int(n_train_examples*valid_ratio)\n",
    "\n",
    "\n",
    "train_data, test_data = data.random_split(data_frame, \n",
    "                                           [n_train_examples, n_test_examples])\n",
    "train_data, valid_data = data.random_split(train_data, \n",
    "                                           [n_train_examples-n_validate_examples, n_validate_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Sciu70hTOGh"
   },
   "source": [
    "Aplicar técnicas para aumentar la cantidad de imágenes en los datos de entrenamiento, recortando las imágenes, girando, modificando los contrastes, entre otras. Como mínimo, se debe utilizar dos técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "TjJrVxCaTTbG"
   },
   "outputs": [],
   "source": [
    "#Tecnica 1: Rotar imagen\n",
    "\n",
    "#Tecnica 2: Recortar imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsa9DYp_TXe-"
   },
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSXQJ_DmTcTN"
   },
   "source": [
    "Creación de una arquitectura desde cero, para esté paso se recomienda adaptar la implementación LeNet vista en clases. Cuya implementación debe funcionar tanto en CPU como en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "jYfHw9JiTacJ"
   },
   "outputs": [],
   "source": [
    "#Se crean los dataloaders para procesar los datos a travez de las iteraciones.\n",
    "#Se trabaja con un tamaño de lote BATCH_SIZE\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                 batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, \n",
    "                               out_channels = 6, \n",
    "                               kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, \n",
    "                               out_channels = 16, \n",
    "                               kernel_size = 5)\n",
    "        self.fc_1 = nn.Linear(44944, 120)\n",
    "        self.fc_2 = nn.Linear(120, 84)\n",
    "        self.fc_3 = nn.Linear(84, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = [batch size, 1, 28, 28]\n",
    "        x = self.conv1(x)\n",
    "        #x = [batch size, 6, 24, 24]\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        #x = [batch size, 6, 12, 12]\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = [batch size, 16, 8, 8]\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        #x = [batch size, 16, 4, 4]\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape[0]) \n",
    "        #torch.Size([64, 16, 4, 4]) 64 vectore de cada uno 256 pixeles [16 + 16 + 16 + 16 .... + 16]\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(x.shape) \n",
    "        #torch.Size([64, 256])\n",
    "        #x = [batch size, 16*4*4 = 256]\n",
    "        h = x\n",
    "        x = self.fc_1(x)\n",
    "        \n",
    "        #x = [batch size, 120]\n",
    "        \n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = batch size, 84]\n",
    "        \n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        #x = [batch size, output dim]\n",
    "        \n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 10\n",
    "\n",
    "model = LeNet(OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        #y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred, _ = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            #y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-5d08398568d7>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y).to(device)\n",
      "<ipython-input-52-ec60f0ed50f8>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 1.975 | Train Acc: 21.14%\n",
      "\t Val. Loss: 1.693 |  Val. Acc: 12.50%\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.647 | Train Acc: 19.00%\n",
      "\t Val. Loss: 1.676 |  Val. Acc: 12.50%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-ec60f0ed50f8>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.650 | Test Acc: 15.81%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96LT9qJOTffR"
   },
   "source": [
    "Probar el modelo por lo menos con tres configuraciones de parámetros, entre los que se puede variar número de convoluciones, función de activación, número de neuronas en capas densas, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ke_6fn0dTgDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz6YH9nUTiTh"
   },
   "source": [
    "Crear los gráficos de entrenamiento, donde se vea el error en los conjuntos de train y test durante cada época de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMDTcbRGTilv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLd10RZTTkZ4"
   },
   "source": [
    "Crear matriz de clasificación del modelo que presento el menor error en la clasificación, por lo tanto la mejor clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B89BJBXxTkwO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1kV-wKBTl-j"
   },
   "source": [
    "### Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5NZW3OeTqaZ"
   },
   "source": [
    "Importar una arquitectura profunda que se encuentra pre-entrenada con ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urLzcjxdTssf"
   },
   "source": [
    "Realizar la modificación en la arquitectura para poder clasificar el número de clases que usted haya escogido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "1K0ooYnUTuoA"
   },
   "outputs": [],
   "source": [
    "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 5)\n",
    "nn.init.xavier_uniform_(finetune_net.fc.weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcjNzuGcTwSr"
   },
   "source": [
    "Aplicar Fine-tuning en la arquitectura profunda, para re-entrenar con el conjunto de datos seleccionado manteniendo los pesos de las arquitecturas ResNet, VGG o DenseNet. Cuya implementación creada debe funcionar tanto en CPU como en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTFgodbLTwnv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjYagqQQT0ap"
   },
   "source": [
    "Probar el modelo por lo menos con tres configuraciones de parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ3RkqoTT1UB"
   },
   "source": [
    "Crear los gráficos de entrenamiento, donde se vea el error en los conjuntos de train y test durante cada época de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQmE4i4IT29_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k2g71iKT4nA"
   },
   "source": [
    "Crear matriz de clasificación del modelo que presento el menor error en la clasificación, por lo tanto la mejor clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_4XRqEIT490"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab1  - Deep",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
