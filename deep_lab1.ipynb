{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46uESFa1Shq6"
   },
   "source": [
    "### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNLmW5LIStb6"
   },
   "source": [
    "Importar las bibliotecas que son usadas en el desarrollo del laboratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usar Python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install d2l==0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install 'h5py==2.10.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Bibliotecas propias\n",
    "%matplotlib inline\n",
    "import os,os.path,math,scipy\n",
    "import PIL\n",
    "\n",
    "from datasets import ClassLabel\n",
    "\n",
    "#Bibliotecas para LeNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se setean las seeds\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TZd8RYTlS4rb"
   },
   "outputs": [],
   "source": [
    "### Escribir en estos bloques el codigo de acorde a las instrucciones planteadas\n",
    "### pero no olvide que puede crear los bloques que quiera y puede agregar las sub secciones \n",
    "### que deseen, esto es solo una guía basica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIdsSOfqS7XW"
   },
   "source": [
    "Lectura del conjunto de datos, deberá diseñar una estrategia para la selección y lectura del sub conjunto de datos, es decir, se debe crear un conjuntos de datos pequeño que contenga entre 3 a 5 clases y una cantidad de $N$ ejemplos para cada clase, con el fin de tener un conjunto de datos balanceado con un tamaño que se pueda trabajar con los recursos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_amount_of_n_images(img_dir_list, N):\n",
    "    for img_dir in img_dir_list:\n",
    "        img_count = len([name for name in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, name))])\n",
    "        if(img_count < N):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_images(data_frame ,img_label,img_dir):\n",
    "    count = 0\n",
    "    for img in os.listdir(img_dir):\n",
    "        if(count < N):\n",
    "            path = os.path.join(img_dir,img)\n",
    "            #img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "            #img = cv2.resize(img,(TAM_MAX,TAM_MAX))\n",
    "            #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = PIL.Image.open(path)\n",
    "            img = img.resize((TAM_MAX,TAM_MAX))\n",
    "            #img = img[:3]\n",
    "            data_frame.append((img,img_label))\n",
    "            count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Direccion para carpetas de clases\n",
    "chihuahua_dir = 'images/n02085620-Chihuahua'\n",
    "japanese_spaniel_dir = 'images/n02085782-Japanese_spaniel'\n",
    "maltese_dir = 'images/n02085936-Maltese_dog'\n",
    "shihtzu_dir = 'images/n02086240-Shih-Tzu'\n",
    "blenheim_spaniel_dir = 'images/n02086646-Blenheim_spaniel'\n",
    "aux_dir_list = [chihuahua_dir,japanese_spaniel_dir,maltese_dir,shihtzu_dir,blenheim_spaniel_dir]\n",
    "\n",
    "#Se define la cantidad N de ejemplos por clase\n",
    "N = 140\n",
    "\n",
    "#Se define el tamaño maximo de imagen\n",
    "TAM_MAX = 224\n",
    "\n",
    "#Data frame para almacenar las tuplas (imagen,label)\n",
    "data_frame = []\n",
    "\n",
    "#Se modifica el nombre de los labels\n",
    "labels_names = ['chihuahua','japanese_spaniel','maltese','shihtzu','blenheim_spaniel']\n",
    "\n",
    "#Se verifica que todos los directorios tengan la misma cantidad N de imagenes.\n",
    "if(same_amount_of_n_images(aux_dir_list,N)):\n",
    "    #Se almacenan las imagenes, y los labels\n",
    "    storage_images(data_frame,1,chihuahua_dir)\n",
    "    storage_images(data_frame,2,japanese_spaniel_dir)\n",
    "    storage_images(data_frame,3,maltese_dir)\n",
    "    storage_images(data_frame,4,shihtzu_dir)\n",
    "    storage_images(data_frame,5,blenheim_spaniel_dir)\n",
    "else:\n",
    "    print('N invalido')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fq4HhEI-TICF"
   },
   "source": [
    "Realizar la división de los datos en dos conjuntos de datos, Train (70%) y Test (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "\n",
    "n_train_examples = int(len(data_frame) * train_ratio)\n",
    "n_test_examples = len(data_frame) - n_train_examples\n",
    "\n",
    "train_data, test_data = data.random_split(data_frame, \n",
    "                                           [n_train_examples, n_test_examples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se definen las transformaciones para las entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se aplican las transformaciones en los datos de validacion y entranamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aux_train_data = train_data\n",
    "train_data = []\n",
    "for i in aux_train_data:\n",
    "    image_x, label_y = next(iter(aux_train_data))\n",
    "    image_x = train_augs(image_x)\n",
    "    train_data.append((image_x,label_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crean los datos de validacion a partir de los datos de entrenamiento.\n",
    "#Se utilizan el 10% de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_data)  # total number of examples\n",
    "n_valid = int(0.1 * n)  # take ~10% for test\n",
    "valid_data = torch.utils.data.Subset(train_data, range(n_valid))  # take first 10%\n",
    "train_data = torch.utils.data.Subset(train_data, range(n_valid, n))  # take the rest     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De igual forma para los datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_test_data = test_data\n",
    "test_data = []\n",
    "for i in aux_test_data:\n",
    "    image_x, label_y = next(iter(aux_test_data))\n",
    "    image_x = test_augs(image_x)\n",
    "    test_data.append((image_x,label_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Sciu70hTOGh"
   },
   "source": [
    "Aplicar técnicas para aumentar la cantidad de imágenes en los datos de entrenamiento, recortando las imágenes, girando, modificando los contrastes, entre otras. Como mínimo, se debe utilizar dos técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TjJrVxCaTTbG"
   },
   "outputs": [],
   "source": [
    "#Tecnica 1: Rotar imagen\n",
    "\n",
    "#data_augmentation = tf.keras.Sequential([\n",
    "#  tf.keras.layers.RandomFlip('horizontal'),\n",
    "#  tf.keras.layers.RandomRotation(0.2),\n",
    "#])\n",
    "\n",
    "#Tecnica 2: Recortar imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsa9DYp_TXe-"
   },
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSXQJ_DmTcTN"
   },
   "source": [
    "Creación de una arquitectura desde cero, para esté paso se recomienda adaptar la implementación LeNet vista en clases. Cuya implementación debe funcionar tanto en CPU como en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jYfHw9JiTacJ"
   },
   "outputs": [],
   "source": [
    "#Se crean los dataloaders para procesar los datos a travez de las iteraciones.\n",
    "#Se trabaja con un tamaño de lote BATCH_SIZE\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                 batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, \n",
    "                               out_channels = 6, \n",
    "                               kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, \n",
    "                               out_channels = 16, \n",
    "                               kernel_size = 5)\n",
    "        self.fc_1 = nn.Linear(44944, 120)\n",
    "        self.fc_2 = nn.Linear(120, 84)\n",
    "        self.fc_3 = nn.Linear(84, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Primera capa\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Segunda capa\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        h = x\n",
    "        x = self.fc_1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 10\n",
    "\n",
    "model = LeNet(OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        #y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred, _ = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            #y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascoronado/opt/anaconda3/envs/python3-7/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/matiascoronado/opt/anaconda3/envs/python3-7/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.429 | Train Acc: 85.71%\n",
      "\t Val. Loss: 0.000 |  Val. Acc: 100.00%\n",
      "Epoch: 02 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.000 |  Val. Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascoronado/opt/anaconda3/envs/python3-7/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 58.620 | Test Acc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96LT9qJOTffR"
   },
   "source": [
    "Probar el modelo por lo menos con tres configuraciones de parámetros, entre los que se puede variar número de convoluciones, función de activación, número de neuronas en capas densas, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ke_6fn0dTgDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz6YH9nUTiTh"
   },
   "source": [
    "Crear los gráficos de entrenamiento, donde se vea el error en los conjuntos de train y test durante cada época de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMDTcbRGTilv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLd10RZTTkZ4"
   },
   "source": [
    "Crear matriz de clasificación del modelo que presento el menor error en la clasificación, por lo tanto la mejor clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B89BJBXxTkwO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1kV-wKBTl-j"
   },
   "source": [
    "### Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo dogs breeds\n",
    "# https://www.tensorflow.org/tutorials/images/transfer_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5NZW3OeTqaZ"
   },
   "source": [
    "Importar una arquitectura profunda que se encuentra pre-entrenada con ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 04:37:15.141813: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-07 04:37:15.142713: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "TAM_MAX = 224\n",
    "IMG_SIZE = IMG_SIZE = (TAM_MAX, TAM_MAX)\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_net.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urLzcjxdTssf"
   },
   "source": [
    "Realizar la modificación en la arquitectura para poder clasificar el número de clases que usted haya escogido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "1K0ooYnUTuoA"
   },
   "outputs": [],
   "source": [
    "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 5)\n",
    "nn.init.xavier_uniform_(finetune_net.fc.weight);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=44944, bias=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_net.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcjNzuGcTwSr"
   },
   "source": [
    "Aplicar Fine-tuning en la arquitectura profunda, para re-entrenar con el conjunto de datos seleccionado manteniendo los pesos de las arquitecturas ResNet, VGG o DenseNet. Cuya implementación creada debe funcionar tanto en CPU como en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjYagqQQT0ap"
   },
   "source": [
    "Probar el modelo por lo menos con tres configuraciones de parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ3RkqoTT1UB"
   },
   "source": [
    "Crear los gráficos de entrenamiento, donde se vea el error en los conjuntos de train y test durante cada época de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQmE4i4IT29_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k2g71iKT4nA"
   },
   "source": [
    "Crear matriz de clasificación del modelo que presento el menor error en la clasificación, por lo tanto la mejor clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_4XRqEIT490"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab1  - Deep",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
